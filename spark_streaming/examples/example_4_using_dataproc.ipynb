{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark_streaming/examples/example_4_using_dataproc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "b805aca4-2d12-47de-d985-2b8a22eeb565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZdHGoFyTlMt"
      },
      "source": [
        "## Simulate producer:\n",
        "- extract data from API\n",
        "- store data as json in the lake\n",
        "- run task async"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTQhp8UbFUCl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "import datetime\n",
        "import asyncio\n",
        "\n",
        "landing_path=f\"gs://{bucket_name}/datalake/landing/{table_path}\"\n",
        "\n",
        "async def ingest_from_api(url: str, table: str, schema: StructType = None):\n",
        "  response = requests.get(url)\n",
        "  timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "  if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    with open(f\"{landing_path}_{int(timestamp)}.json\", \"w\") as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "async def producer(loop: int, interval_time: int):\n",
        "  for i in range(loop):\n",
        "    await ingest_from_api(\"https://api.carrismetropolitana.pt/vehicles\", \"vehicles\")\n",
        "    await ingest_from_api(\"https://api.carrismetropolitana.pt/lines\", \"lines\")\n",
        "    await asyncio.sleep(interval_time)\n",
        "\n",
        "async def main():\n",
        "  asyncio.create_task(producer(10, 30))\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIqHdZEKUEmo"
      },
      "source": [
        "- Read from /content/landing as streaming\n",
        "- store data in memory (for testing)\n",
        "- store data in the bronze layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dTSf527Fhy0"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "def insert_vehicles(df, batch_id, path):\n",
        "  df.write.format(\"parquet\").mode(\"append\").save(path)\n",
        "\n",
        "vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                             StructField('block_id', StringType(), True),\n",
        "                             StructField('current_status', StringType(), True),\n",
        "                             StructField('id', StringType(), True),\n",
        "                             StructField('lat', FloatType(), True),\n",
        "                             StructField('line_id', StringType(), True),\n",
        "                             StructField('lon', FloatType(), True),\n",
        "                             StructField('pattern_id', StringType(), True),\n",
        "                             StructField('route_id', StringType(), True),\n",
        "                             StructField('schedule_relationship', StringType(), True),\n",
        "                             StructField('shift_id', StringType(), True),\n",
        "                             StructField('speed', FloatType(), True),\n",
        "                             StructField('stop_id', StringType(), True),\n",
        "                             StructField('timestamp', TimestampType(), True),\n",
        "                             StructField('trip_id', StringType(), True)])\n",
        "\n",
        "# define paths\n",
        "bucket_name=\"edit-data-eng-dev\"\n",
        "table_path=\"vehicles\"\n",
        "landing_path=f\"gs://{bucket_name}/datalake/landing/{table_path}\"\n",
        "bronze_path=f\"gs://{bucket_name}/datalake/bronze/{table_path}\"\n",
        "\n",
        "stream = spark.readStream.format(\"parquet\").schema(schema).load(landing_path)\n",
        "\n",
        "query = (stream\n",
        "          .writeStream\n",
        "          .outputMode(\"append\")\n",
        "          .foreachBatch(insert_vehicles(bronze_path))\n",
        "          .option(\"checkpointLocation\", \"/content/bronze/checkpoint\")\n",
        "          .trigger(processingTime='20 seconds')\n",
        "          .start()\n",
        "          .awaitTermination(60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/content"
      ],
      "metadata": {
        "id": "rW0A89D6fU0Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "def insert_into_table(df, batch_id):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'name': F.lit(fake.name()),\n",
        "      'address': F.lit(fake.address()),\n",
        "      'email': F.lit(fake.email()),\n",
        "      'dob': F.lit(fake.date_of_birth()),\n",
        "      'phone': F.lit(fake.phone_number())\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  df.write.mode(\"append\").format(\"parquet\").save(\"content/output/events\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_into_table)\n",
        ".start()\n",
        ".awaitTermination(20)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "9uQNlSwkZYee",
        "outputId": "19673e9f-4e85-45b4-e898-7adb318ea1cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=41>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o13.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JError",
          "evalue": "An error occurred while calling o35.awaitTermination",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-636b18f39b92>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minsert_into_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    217\u001b[0m                     \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 )\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n",
            "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o35.awaitTermination"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query"
      ],
      "metadata": {
        "id": "cnqTKcjrZY3A",
        "outputId": "e3e1308e-fe5c-4ad4-fa31-05866764bc30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/output/events\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "c4BGJlctZdon",
        "outputId": "4698e21d-a038-421c-85b9-2f250ad3b36e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+-----------------+--------------------+--------------------+----------+--------------------+\n",
            "|           timestamp|value|             name|             address|               email|       dob|               phone|\n",
            "+--------------------+-----+-----------------+--------------------+--------------------+----------+--------------------+\n",
            "|2024-11-29 20:16:...|   29|     Tanya Thomas|66189 Gonzalez Pa...|edwardsjenna@exam...|1989-11-19| (822)723-3944x10441|\n",
            "|2024-11-29 20:16:...|    0|       Gary Payne|151 Antonio Summi...|kathywalls@exampl...|2023-07-14|  781.561.1601x13391|\n",
            "|2024-11-29 20:16:...|    2|       Gary Payne|151 Antonio Summi...|kathywalls@exampl...|2023-07-14|  781.561.1601x13391|\n",
            "|2024-11-29 20:16:...|   16|     Ann Cummings|53919 William Cou...|michellepatrick@e...|1950-11-21|     +1-334-420-3370|\n",
            "|2024-11-29 20:16:...|    4|   Michelle Myers|1526 Carter Traff...|sanderson@example...|1926-11-24|  (774)770-8398x0088|\n",
            "|2024-11-29 20:16:...|   20|Jessica Miller MD|0494 Johnson Park...|gibsoncassandra@e...|1969-03-01|    785.631.1996x914|\n",
            "|2024-11-29 20:16:...|   24|  Kristina Chavez|4485 Julie Garden...|wellskristen@exam...|1913-07-16|       (264)977-1794|\n",
            "|2024-11-29 20:16:...|   10|      Jose Murphy|10867 Margaret Co...|rogersbeth@exampl...|1957-08-14|  925.616.5210x96317|\n",
            "|2024-11-29 20:16:...|   15|    Megan Andrews|18217 Trevino Cau...|jonathanallison@e...|1928-01-08|001-212-502-3881x...|\n",
            "|2024-11-29 20:16:...|   34|     Matthew Shaw|658 Jackson Mount...|stephanie83@examp...|2007-06-12|  369-972-2983x92448|\n",
            "|2024-11-29 20:16:...|   23|       Jacob Frye|37995 Mclean Junc...|michael98@example...|1965-01-21| +1-268-948-3304x639|\n",
            "|2024-11-29 20:16:...|   12|Jennifer Phillips|79554 Stacey Land...| james03@example.org|1970-05-26|001-543-516-1497x...|\n",
            "|2024-11-29 20:16:...|   19|  Brendan Dickson|750 Gregory Locks...|adkinsdon@example...|1992-01-08|001-753-307-0905x853|\n",
            "|2024-11-29 20:16:...|   35| Stephanie Durham|49546 Wesley Gree...|edwardsmichael@ex...|1950-04-16|        663-346-5284|\n",
            "|2024-11-29 20:16:...|   28|   Timothy Taylor|3792 Williamson T...| pgrimes@example.com|1911-08-31|    372.290.1965x889|\n",
            "|2024-11-29 20:16:...|    5|   Douglas Murray|10880 Clay Center...|kentramos@example...|1976-06-15|001-961-525-3905x...|\n",
            "|2024-11-29 20:16:...|    6|    John Jacobson|64301 Tonya Valle...| phodges@example.com|2019-07-30|001-637-973-4941x...|\n",
            "|2024-11-29 20:16:...|   32|     William Webb|3455 Justin Manor...|  ecurry@example.net|1988-08-04|        667-416-1305|\n",
            "|2024-11-29 20:17:...|   38|     Cameron Pugh|6113 Fisher Stree...| kelly56@example.net|1913-04-15|       (326)711-0256|\n",
            "|2024-11-29 20:16:...|   25|   Alec Jones PhD|59565 Michael Isl...|qmatthews@example...|1954-10-07|          8622989511|\n",
            "+--------------------+-----+-----------------+--------------------+--------------------+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp\n",
        "id\n",
        "message_type (OPEN, RECEIVED, SENT, CREATED)\n",
        "message_id\n",
        "user"
      ],
      "metadata": {
        "id": "XtCo5_4mf8cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop"
      ],
      "metadata": {
        "id": "K8b2hZFwiW5E",
        "outputId": "693c3744-4b5f-4e14-8152-f69f5039795e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method StreamingQuery.stop of <pyspark.sql.streaming.query.StreamingQuery object at 0x7b2b0fd7d600>>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.streaming.query.StreamingQuery.stop</b><br/>def stop() -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py</a>Stop this streaming query.\n",
              "\n",
              ".. versionadded:: 2.0.0\n",
              "\n",
              ".. versionchanged:: 3.5.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; sdf = spark.readStream.format(&quot;rate&quot;).load()\n",
              "&gt;&gt;&gt; sq = sdf.writeStream.format(&#x27;memory&#x27;).queryName(&#x27;this_query&#x27;).start()\n",
              "&gt;&gt;&gt; sq.isActive\n",
              "True\n",
              "\n",
              "Stop streaming query\n",
              "\n",
              "&gt;&gt;&gt; sq.stop()\n",
              "\n",
              "&gt;&gt;&gt; sq.isActive\n",
              "False</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 349);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "def insert_into_table(df, batch_id):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED'))),\n",
        "      'event_id': F.lit(fake.uuid4()),\n",
        "      'country': F.lit(fake.country()),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  df.write.mode(\"append\").format(\"parquet\").save(\"content/output/messages\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_into_table)\n",
        ".start()\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "tPCOdivrfhYh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/output/messages\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "outputId": "a1715216-c141-45f2-cb2e-51ac815fdf49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+--------------------+-------+\n",
            "|           timestamp|value|event_type|            event_id|             country|user_id|\n",
            "+--------------------+-----+----------+--------------------+--------------------+-------+\n",
            "|2024-11-29 20:22:...|   53|  RECEIVED|668615df-35aa-404...|United States Vir...|   1022|\n",
            "|2024-11-29 20:22:...|   29|  RECEIVED|1cee98fd-7f3e-410...|French Southern T...|   1010|\n",
            "|2024-11-29 20:22:...|   36|  RECEIVED|885b1c38-e17c-471...|Northern Mariana ...|   1008|\n",
            "|2024-11-29 20:22:...|   43|      OPEN|66e23e9f-ccc5-407...|French Southern T...|   1035|\n",
            "|2024-11-29 20:22:...|   39|  RECEIVED|7fa12a91-3a66-488...|Libyan Arab Jamah...|   1010|\n",
            "|2024-11-29 20:22:...|   32|   CREATED|4007c051-5ebb-489...|British Virgin Is...|   1040|\n",
            "|2024-11-29 20:22:...|   22|   CLICKED|67fb1601-b897-439...|Libyan Arab Jamah...|   1004|\n",
            "|2024-11-29 20:22:...|   28|   CLICKED|8946a4e6-36a1-475...|Saint Kitts and N...|   1010|\n",
            "|2024-11-29 20:22:...|   51|      OPEN|3aa733e9-b571-46a...|United States of ...|   1013|\n",
            "|2024-11-29 20:21:...|    0|      SENT|84b9aabb-09b2-4af...|              Cyprus|   1050|\n",
            "|2024-11-29 20:21:...|    2|      SENT|84b9aabb-09b2-4af...|              Cyprus|   1050|\n",
            "|2024-11-29 20:22:...|   21|      SENT|410a5328-18b4-49b...| Trinidad and Tobago|   1029|\n",
            "|2024-11-29 20:22:...|   37|      OPEN|3a9b8369-acd9-4f1...| Antigua and Barbuda|   1049|\n",
            "|2024-11-29 20:22:...|   41|      SENT|f971fd4b-95bf-40e...| Antigua and Barbuda|   1029|\n",
            "|2024-11-29 20:21:...|    6|      OPEN|34d13caf-44a1-4b9...|    Pitcairn Islands|   1050|\n",
            "|2024-11-29 20:22:...|   52|      SENT|7da5c9d1-bf06-436...|    French Polynesia|   1020|\n",
            "|2024-11-29 20:21:...|    8|   CLICKED|826391f3-b4ed-413...|       Cote d'Ivoire|   1025|\n",
            "|2024-11-29 20:21:...|    7|  RECEIVED|2e587697-41e5-4cd...|         Puerto Rico|   1033|\n",
            "|2024-11-29 20:22:...|   42|   CLICKED|bbb1d481-6003-4d5...|        Sierra Leone|   1003|\n",
            "|2024-11-29 20:22:...|   25|   CLICKED|59c6f98b-9dea-414...|          Luxembourg|   1027|\n",
            "+--------------------+-----+----------+--------------------+--------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZAHIZeZMlpoH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
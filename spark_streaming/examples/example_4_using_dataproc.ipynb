{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark_streaming/examples/example_4_using_dataproc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "b805aca4-2d12-47de-d985-2b8a22eeb565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context\n",
        "- Message events are coming from platform message broker (kafka, pubsub, kinesis...)\n",
        "- You need to process the data according to the requirements\n",
        "\n",
        "## Challenge 1 (Streaming)\n",
        "Step 1:\n",
        "- Change writeStream (cell \"Producer\") to partition data by \"date\" column\n",
        "  - \"date\" column must be created from \"timestamp\"\n",
        "- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "- Delete /content/lake/bronze/messages and reprocess data\n",
        "  - For reprocessing, run the streaming for at least 1 minute\n",
        "\n",
        "Step 2:\n",
        "- Implement new stream job to read from messages (parquet)\n",
        "- Identify corrupted data and write into another location as PARQUET\n",
        "  - logic: event_status is null, empty or equal to \"NONE\"\n",
        "  - location: /content/lake/bronze/messages_corrupted/data\n",
        "  - checkpoint: /content/lake/bronze/messages_corrupted/checkpoint\n",
        "  - use StructSchema\n",
        "  - Set trigger interval to 10 seconds\n",
        "- For reprocessing, run the streaming for at least 60 seconds\n",
        "\n",
        "------------------\n",
        "\n",
        "## Challenge 2 (Streaming)\n",
        "- Business reporting\n",
        "- Aggregate events by event_status & date\n",
        "\n",
        "### Technical requirements\n",
        "- Implement writeStreaming job to write output as PARQUET\n",
        "  - location: /content/lake/gold/events_daily\n",
        "  - Partition data by date\n",
        "  - Write into gold layer\n",
        "\n",
        "-------------------\n",
        "\n",
        "## Challenge 3 (Reporting / Batching)\n",
        "- Implement reporting to identify anomalies\n",
        "\n"
      ],
      "metadata": {
        "id": "Rcybt71kTDNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp\n",
        "id\n",
        "message_type (OPEN, RECEIVED, SENT, CREATED)\n",
        "message_id\n",
        "user"
      ],
      "metadata": {
        "id": "XtCo5_4mf8cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install faker"
      ],
      "metadata": {
        "id": "Udk3tohSaXOH",
        "outputId": "82f99681-0b2f-4f1b-f56c-2dfe05dd3d4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Downloading Faker-33.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls content/output/messages/date=2024-11-30 | wc"
      ],
      "metadata": {
        "id": "evcoEw27b1dl",
        "outputId": "702a1c2a-f7f8-41e6-83e4-8c56515bbfc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    151     151   10268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf content/output/"
      ],
      "metadata": {
        "id": "aFdP7uDQdbWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "50 users\n",
        "100 messages"
      ],
      "metadata": {
        "id": "QI4swNMZcRHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CpxNS1cAbcIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Producer"
      ],
      "metadata": {
        "id": "cDGMKwBdi1qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "def enrich_data(df):\n",
        "  fake = Faker()\n",
        "  messages = [fake.uuid4() for _ in range(99)]\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").partitionBy(\"date\").format(\"parquet\").save(\"content/lake/bronze/messages\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "df_transformed = df_stream.withColumn(\"date\", F.to_date(F.col(\"timestamp\")))\n",
        "\n",
        "# write stream\n",
        "query = (df_transformed.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n"
      ],
      "metadata": {
        "id": "tPCOdivrfhYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7416be69-0d03-410a-97d6-5ba937119bf7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "KNyUK3yplDhg",
        "outputId": "33c42426-d1cb-4f49-fbd1-1dc7c0d7500c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
            "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
            "  File \"<ipython-input-2-7e88871c2524>\", line 24, in insert_messages\n",
            "    enrich.write.mode(\"append\").partitionBy(\"date\").format(\"parquet\").save(\"content/lake/bronze/messages\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\", line 1463, in save\n",
            "    self._jwrite.save(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o2861.save.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:980)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy30.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/*\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWQExsnzlMFe",
        "outputId": "f5989d6a-04e6-464b-c064-ed223a53ce80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2024-12-02 23:06:...|    0|          |fb3079f9-336d-439...|  OTHER|      2001|   1044|\n",
            "|2024-12-02 23:06:...|    2|          |fb3079f9-336d-439...|  OTHER|      2001|   1044|\n",
            "|2024-12-02 23:06:...|    4|          |fb3079f9-336d-439...|  OTHER|      2001|   1044|\n",
            "|2024-12-02 23:06:...|    1|          |fb3079f9-336d-439...|  OTHER|      2001|   1044|\n",
            "|2024-12-02 23:06:...|    3|          |fb3079f9-336d-439...|  OTHER|      2001|   1044|\n",
            "|2024-12-02 23:06:...|    5|          |24436b0e-d45c-4ef...|    SMS|      2006|   1050|\n",
            "|2024-12-02 23:06:...|    7|          |24436b0e-d45c-4ef...|    SMS|      2006|   1050|\n",
            "|2024-12-02 23:08:...|  105|  RECEIVED|1328c8b4-1232-4f8...|  OTHER|      2000|   1042|\n",
            "|2024-12-02 23:09:...|  140|  RECEIVED|1c61b2c8-e233-4ab...|  OTHER|      2002|   1033|\n",
            "|2024-12-02 23:08:...|   77|  RECEIVED|11dc2ce7-2656-4fb...|  OTHER|      2014|   1034|\n",
            "|2024-12-02 23:07:...|   25|  RECEIVED|ddf55c4a-fbe0-41a...|  EMAIL|      2015|   1050|\n",
            "|2024-12-02 23:08:...|   89|  RECEIVED|657253da-c438-457...|  OTHER|      2002|   1046|\n",
            "|2024-12-02 23:08:...|   93|   CREATED|0bc6d712-ab85-459...|  EMAIL|      2006|   1039|\n",
            "|2024-12-02 23:08:...|  115|  RECEIVED|e53be143-eaf3-4da...|   PUSH|      2000|   1049|\n",
            "|2024-12-02 23:07:...|   33|  RECEIVED|ac9ae710-8633-471...|   PUSH|      2004|   1047|\n",
            "|2024-12-02 23:07:...|   38|  RECEIVED|9d09f10f-0907-46d...|   CHAT|      2014|   1038|\n",
            "|2024-12-02 23:08:...|  126|  RECEIVED|0974c29f-59e0-4e3...|   PUSH|      2008|   1002|\n",
            "|2024-12-02 23:08:...|   84|   CREATED|61de0e42-6762-44a...|  EMAIL|      2010|   1014|\n",
            "|2024-12-02 23:07:...|   70|  RECEIVED|575a7db9-7722-405...|   CHAT|      2013|   1032|\n",
            "|2024-12-02 23:07:...|   22|  RECEIVED|2f8120c5-939a-421...|   PUSH|      2007|   1048|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming Messages Corrupted"
      ],
      "metadata": {
        "id": "swvPj9hVpzNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "def insert_messages_corrupted(df: DataFrame, batch_id):\n",
        "  df.write.mode(\"append\").partitionBy(\"date\").format(\"parquet\").save(\"content/lake/silver/messages_corrupted\")\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True), StructField('value', LongType(), True), StructField('event_type', StringType(), True), StructField('message_id', StringType(), True), StructField('channel', StringType(), True), StructField('country_id', IntegerType(), True), StructField('user_id', IntegerType(), True), StructField('date', DateType(), True)])\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"parquet\").schema(schema).load(\"content/lake/bronze/messages/*\")\n",
        "\n",
        "df_corrupted = df_stream.filter(F.col('event_type').isin('NONE', '') | F.col('event_type').isNull())\n",
        "\n",
        "# write stream\n",
        "query = (df_corrupted.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='5 seconds')\n",
        ".foreachBatch(insert_messages_corrupted)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(20)"
      ],
      "metadata": {
        "id": "ZAHIZeZMlpoH",
        "outputId": "7cee646b-b5a8-4148-8a2a-d97de93b14bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "TinO47Q6molI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages_corrupted\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "nk8seEvbmvcU",
        "outputId": "64f8ddd4-0204-447f-db50-c64fb44d2ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+----+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|date|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+----+\n",
            "|2024-12-02 23:07:...|   24|          |0b009667-bcf3-427...|  EMAIL|      2009|   1037|NULL|\n",
            "|2024-12-02 23:07:...|   36|          |e3a66a94-2ff1-4cc...|  OTHER|      2006|   1013|NULL|\n",
            "|2024-12-02 23:06:...|   10|          |67ca029c-ae87-43f...|  OTHER|      2011|   1033|NULL|\n",
            "|2024-12-02 23:07:...|   41|          |fc6a6c21-25a4-4b4...|  EMAIL|      2014|   1041|NULL|\n",
            "|2024-12-02 23:08:...|  130|          |cca8ab48-df16-45b...|  OTHER|      2001|   1000|NULL|\n",
            "|2024-12-02 23:06:...|   13|          |7e16a974-29ee-404...|   CHAT|      2013|   1049|NULL|\n",
            "|2024-12-02 23:07:...|   28|          |391c327c-0303-430...|   CHAT|      2015|   1020|NULL|\n",
            "|2024-12-02 23:07:...|   49|          |d180b858-19fa-44a...|   PUSH|      2010|   1022|NULL|\n",
            "|2024-12-02 23:08:...|  129|          |832834dc-b891-47c...|   CHAT|      2010|   1012|NULL|\n",
            "|2024-12-02 23:09:...|  145|          |75f4ced4-1113-476...|   PUSH|      2000|   1012|NULL|\n",
            "|2024-12-02 23:08:...|   94|          |a7791623-3f7f-400...|   PUSH|      2004|   1022|NULL|\n",
            "|2024-12-02 23:08:...|  128|          |bc2b1957-1b2c-4a3...|   CHAT|      2009|   1000|NULL|\n",
            "|2024-12-02 23:06:...|    6|          |24436b0e-d45c-4ef...|    SMS|      2006|   1050|NULL|\n",
            "|2024-12-02 23:08:...|   81|          |075db54a-fab6-4a5...|    SMS|      2002|   1004|NULL|\n",
            "|2024-12-02 23:07:...|   47|          |e1ed0bd1-b3f7-4f2...|    SMS|      2005|   1003|NULL|\n",
            "|2024-12-02 23:08:...|   79|      NONE|9945eeb6-68ba-471...|   PUSH|      2011|   1039|NULL|\n",
            "|2024-12-02 23:08:...|   80|      NONE|83b49ec2-7ab4-48b...|   PUSH|      2004|   1020|NULL|\n",
            "|2024-12-02 23:08:...|   83|      NONE|bfb9aa33-eaa0-4c7...|   CHAT|      2003|   1039|NULL|\n",
            "|2024-12-02 23:08:...|   96|      NONE|1a14c575-8b72-4f9...|   CHAT|      2015|   1009|NULL|\n",
            "|2024-12-02 23:08:...|  123|      NONE|617f8778-a88a-416...|   PUSH|      2004|   1037|NULL|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fy2f1j9nWBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
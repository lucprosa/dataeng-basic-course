{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark_streaming/examples/example_4_using_dataproc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "b805aca4-2d12-47de-d985-2b8a22eeb565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context\n",
        "- Message events are coming from platform message broker (kafka, pubsub, kinesis...)\n",
        "- You need to process the data according to the requirements\n",
        "\n",
        "## Challenge 1 (Streaming)\n",
        "Step 1:\n",
        "- Change writeStream to partition data by date column\n",
        "- Change location to /content/lake/bronze/messages/data\n",
        "- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "- Delete /content/lake/bronze/messages and reprocess data\n",
        "\n",
        "Step 2:\n",
        "- Implement new stream job to read from messages\n",
        "- Identify corrupted data and write into another location as PARQUET\n",
        "  - logic: event_status is null, empty or equal to \"NONE\"\n",
        "  - location: /content/lake/bronze/messages_corrupted\n",
        "\n",
        "------------------\n",
        "\n",
        "## Challenge 2 (Streaming)\n",
        "- Business reporting\n",
        "- Aggregate events by event_status & date\n",
        "\n",
        "### Technical requirements\n",
        "- Implement writeStreaming job to write output as PARQUET\n",
        "  - location: /content/lake/gold/events_daily\n",
        "  - Partition data by date\n",
        "  - Write into gold layer\n",
        "\n",
        "-------------------\n",
        "\n",
        "## Challenge 3 (Reporting / Batching)\n",
        "- Implement reporting to identify anomalies\n",
        "\n"
      ],
      "metadata": {
        "id": "Rcybt71kTDNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp\n",
        "id\n",
        "message_type (OPEN, RECEIVED, SENT, CREATED)\n",
        "message_id\n",
        "user"
      ],
      "metadata": {
        "id": "XtCo5_4mf8cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install faker"
      ],
      "metadata": {
        "id": "Udk3tohSaXOH",
        "outputId": "f3dda6a6-5005-491a-f184-673f10d935d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Downloading Faker-33.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls content/output/messages/date=2024-11-30 | wc"
      ],
      "metadata": {
        "id": "evcoEw27b1dl",
        "outputId": "702a1c2a-f7f8-41e6-83e4-8c56515bbfc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    151     151   10268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf content/output/"
      ],
      "metadata": {
        "id": "aFdP7uDQdbWp"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "50 users\n",
        "100 messages"
      ],
      "metadata": {
        "id": "QI4swNMZcRHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CpxNS1cAbcIg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Producer"
      ],
      "metadata": {
        "id": "cDGMKwBdi1qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "def enrich_data(df):\n",
        "  fake = Faker()\n",
        "  messages = [fake.uuid4() for _ in range(99)]\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").partitionBy(\"date\").format(\"parquet\").save(\"content/lake/bronze/messages\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "df_transformed = df_stream.withColumn(\"date\", F.to_date(F.col(\"timestamp\")))\n",
        "\n",
        "# write stream\n",
        "query = (df_transformed.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n"
      ],
      "metadata": {
        "id": "tPCOdivrfhYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bcc6851-ab49-4b3b-d618-416703cd1e95"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/*\")\n",
        "df.where(\"value = 59\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWQExsnzlMFe",
        "outputId": "587d9c89-dccb-4079-af0d-8e00d3459094"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2024-11-30 01:12:...|   59|      NONE|55a4fa9e-b3f0-43e...|  EMAIL|      2008|   1012|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming Messages Corrupted"
      ],
      "metadata": {
        "id": "swvPj9hVpzNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "def insert_messages_corrupted(df: DataFrame, batch_id):\n",
        "  df.write.mode(\"append\").partitionBy(\"date\").format(\"parquet\").save(\"content/lake/silver/messages_corrupted\")\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True), StructField('value', LongType(), True), StructField('event_type', StringType(), True), StructField('message_id', StringType(), True), StructField('channel', StringType(), True), StructField('country_id', IntegerType(), True), StructField('user_id', IntegerType(), True), StructField('date', DateType(), True)])\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"parquet\").schema(schema).load(\"content/output/messages/*\")\n",
        "\n",
        "df_corrupted = df_stream.filter(F.col('event_type').isin('NONE', '') | F.col('event_type').isNull())\n",
        "\n",
        "# write stream\n",
        "query = (df_corrupted.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='5 seconds')\n",
        ".foreachBatch(insert_messages_corrupted)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(20)"
      ],
      "metadata": {
        "id": "ZAHIZeZMlpoH"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "TinO47Q6molI"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/output/messages_corrupted\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "nk8seEvbmvcU",
        "outputId": "3743f731-2d6a-47f4-8711-d9c200400800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+----+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|date|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+----+\n",
            "|2024-11-30 01:12:...|   59|      NONE|55a4fa9e-b3f0-43e...|  EMAIL|      2008|   1012|NULL|\n",
            "|2024-11-30 01:11:...|   10|      NONE|8a0b611a-ab58-4e5...|  EMAIL|      2010|   1025|NULL|\n",
            "|2024-11-30 01:11:...|   12|      NONE|915e337d-a631-442...|  OTHER|      2004|   1024|NULL|\n",
            "|2024-11-30 01:12:...|   37|      NONE|a3437da1-d163-47c...|  OTHER|      2010|   1048|NULL|\n",
            "|2024-11-30 01:11:...|    1|      NONE|0bb83df9-fc91-4e4...|   PUSH|      2004|   1037|NULL|\n",
            "|2024-11-30 01:11:...|    7|      NONE|424d4cc4-992f-451...|   PUSH|      2011|   1009|NULL|\n",
            "|2024-11-30 01:11:...|   26|      NONE|1addf7a6-cc13-4b1...|   PUSH|      2008|   1004|NULL|\n",
            "|2024-11-30 01:11:...|   31|      NONE|88adfa8c-2060-458...|   PUSH|      2009|   1001|NULL|\n",
            "|2024-11-30 01:12:...|   40|      NONE|210bc1c9-2cce-49d...|   CHAT|      2007|   1010|NULL|\n",
            "|2024-11-30 01:12:...|   43|      NONE|527b9fa0-ed4f-4e0...|   PUSH|      2011|   1017|NULL|\n",
            "|2024-11-30 01:11:...|   30|      NONE|2f29df1a-88d1-4b2...|   CHAT|      2001|   1014|NULL|\n",
            "|2024-11-30 01:12:...|   36|      NONE|41e727b7-0279-41a...|    SMS|      2006|   1025|NULL|\n",
            "|2024-11-30 01:12:...|   48|      NONE|cba12749-edbc-4bf...|    SMS|      2015|   1020|NULL|\n",
            "|2024-11-30 01:11:...|   28|          |cf2c1f19-b9e2-401...|  EMAIL|      2003|   1011|NULL|\n",
            "|2024-11-30 01:12:...|   56|          |d85a6971-ecc0-46f...|  EMAIL|      2009|   1023|NULL|\n",
            "|2024-11-30 01:12:...|   38|          |c71337e9-6634-4e9...|  OTHER|      2002|   1046|NULL|\n",
            "|2024-11-30 01:11:...|    3|          |5a149224-21dc-442...|   CHAT|      2003|   1042|NULL|\n",
            "|2024-11-30 01:11:...|    8|          |181928cf-476f-48b...|   PUSH|      2006|   1003|NULL|\n",
            "|2024-11-30 01:12:...|   41|          |ca2b0d99-c5a1-44b...|   CHAT|      2000|   1009|NULL|\n",
            "|2024-11-30 01:11:...|   14|          |995f8ede-4c90-4c9...|   PUSH|      2001|   1034|NULL|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fy2f1j9nWBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}